---
title: "Homework 2"
author: "Your name goes here"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
  pdf_document:
    toc: yes
---




```{r}
#| label: load-libraries
#| echo: false # This option disables the printing of code (only output is displayed).
#| message: false
#| warning: false

library(tidyverse)
library(wbstats)
library(skimr)
library(countrycode)
library(here)
```

# Data Visualisation - Exploration

Now that you've demonstrated your software is setup, and you have the basics of data manipulation, the goal of this assignment is to practice transforming, visualising, and exploring data.

# Exploring credit card fraud

We will be using a dataset with credit card transactions containing legitimate and fraud transactions. Fraud is typically well below 1% of all transactions, so a naive model that predicts that all transactions are legitimate and not fraudulent would have an accuracy of well over 99%-- pretty good, no? (well, not quite as we will see later in the course)

You can read more on credit card fraud on [Credit Card Fraud Detection Using Weighted Support Vector Machine](https://www.scirp.org/journal/paperinformation.aspx?paperid=105944)

The dataset we will use consists of credit card transactions and it includes information about each transaction including customer details, the merchant and category of purchase, and whether or not the transaction was a fraud.

## Obtain the data

The dataset is too large to be hosted on Canvas or Github, so please download it from dropbox <https://www.dropbox.com/sh/q1yk8mmnbbrzavl/AAAxzRtIhag9Nc_hODafGV2ka?dl=0> and save it in your `dsb` repo, under the `data` folder

```{r}
#| echo: false
#| message: false
#| warning: false

card_fraud <- read_csv(here::here("data", "card_fraud.csv"))

glimpse(card_fraud)
```

The data dictionary is as follows

| column(variable)      | description                                 |
|-----------------------|---------------------------------------------|
| trans_date_trans_time | Transaction DateTime                        |
| trans_year            | Transaction year                            |
| category              | category of merchant                        |
| amt                   | amount of transaction                       |
| city                  | City of card holder                         |
| state                 | State of card holder                        |
| lat                   | Latitude location of purchase               |
| long                  | Longitude location of purchase              |
| city_pop              | card holder's city population               |
| job                   | job of card holder                          |
| dob                   | date of birth of card holder                |
| merch_lat             | Latitude Location of Merchant               |
| merch_long            | Longitude Location of Merchant              |
| is_fraud              | Whether Transaction is Fraud (1) or Not (0) |

-   In this dataset, how likely are fraudulent transactions? Generate a table that summarizes the number and frequency of fraudulent transactions per year.

```{r}

card_fraud %>%
  filter(is_fraud == "1") %>% 
  group_by(trans_year) %>% 
  summarise(n=n())
```

-   How much money (in US\$ terms) are fraudulent transactions costing the company? Generate a table that summarizes the total amount of legitimate and fraudulent transactions per year and calculate the % of fraudulent transactions, in US\$ terms.

**Answer**: 1,423,140.0 in 2019 and 651,949.2 in 2020

```{r}

card_fraud %>%
  group_by(trans_year, is_fraud) %>% 
  summarise(transactions=sum(amt)) %>% 
  mutate(perc = transactions/sum(transactions))
  

```

-   Generate a histogram that shows the distribution of amounts charged to credit card, both for legitimate and fraudulent accounts. Also, for both types of transactions, calculate some quick summary statistics.

```{r}

card_fraud %>% 
  #create histogram facet wrap by isfraud condition
  ggplot(aes(x = amt)) +
  #making free scales so histogram visualistion isn't skewed by axis scale of other
  facet_wrap(~ is_fraud, scales = "free") +
  geom_histogram(binwidth = 0.1, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Amounts Charged (Log Scale)",
       x = "Amount",
       y = "Frequency") +
  theme_minimal() +
  #setting x axis to log scale as there are outliers which would otherwise skew the visualisation
  scale_x_log10()
  
#generate summary statistics for count, mean, median, sd, min, and max
summary_stats <- card_fraud %>%
  group_by(is_fraud) %>%
  summarise(
    n = n(),
    mean_amt = mean(amt, na.rm = TRUE),
    median_amt = median(amt, na.rm = TRUE),
    sd_amt = sd(amt, na.rm = TRUE),
    min_amt = min(amt, na.rm = TRUE),
    max_amt = max(amt, na.rm = TRUE)
  )
  # Print summary statistics
  print(summary_stats)



```

-   What types of purchases are most likely to be instances of fraud? Consider category of merchants and produce a bar chart that shows % of total fraudulent transactions sorted in order.

**Answer**: It appears that grocery_pos and shopping_net are the types of purchases which are most likely to be fraudulent transactions.

If considering the highest % of fraudulent transactions as % of total fraudulent transactions, then Shopping_net, Grocery_pos, and Misc_net are the 3 highest.

```{r}

# This shows % of fraudulent transactions by category

card_fraud %>%
  #group by transaction category
  group_by(category, is_fraud) %>% 
  #this will give the count total for each category and fraud status
  summarise(transactions=n()) %>%
  #this will calculate percentage of each category that is fraudulent and non/fraudulent - sum of each category will equal 1
  mutate(perc = transactions/sum(transactions)) %>% 
  filter(is_fraud == 1) %>% 
  arrange(desc(perc))

# This shows % of fraudulent transactions by category as a % of total fraudulent transactions - however this measure might not be accurate, as some categories might have transaction numbers significantly higher than others, so the absolute figures of fraudulent transactions could dwarf other categories

card_fraud %>%
  #remove non-fraudulent transactions
  filter(is_fraud == 1) %>% 
  group_by(category) %>% 
  summarise(transactions=n()) %>% 
  #this will calculate percentage of fraudulent transactions in each category that as proportion of overall fraudulent transactions
  mutate(perc = transactions/sum(transactions)) %>% 
    arrange(desc(perc)) %>% 
  print() %>% 

  #create bar chart 
  ggplot(aes(x = reorder(category, perc), y = perc)) +
  geom_bar(stat = "identity", fill = "blue") +
  labs(title = "Percentage of Total Fraudulent Transactions by Category",
       x = "Category",
       y = "Percentage of Fraudulent Transactions") +
  theme_minimal() +
  #flip coordinate as currently a column chart
  coord_flip()


```

-   When is fraud more prevalent? Which days, months, hours? To create new variables to help you in your analysis, we use the `lubridate` package and the following code

**Answer:**

Fraud is most prevalent on the following:

-   Month: March, May, June, December

-   Day: Sunday and Monday

-   Hour: Hours 22 and 23

```         
mutate(
  date_only = lubridate::date(trans_date_trans_time),
  month_name = lubridate::month(trans_date_trans_time, label=TRUE),
  hour = lubridate::hour(trans_date_trans_time),
  weekday = lubridate::wday(trans_date_trans_time, label = TRUE)
  )
```

```{r}

card_fraud %>%
  #create new variables for date, month, hour, weekday
  mutate(
  date_only = lubridate::date(trans_date_trans_time),
  month_name = lubridate::month(trans_date_trans_time, label=TRUE),
  hour = lubridate::hour(trans_date_trans_time),
  weekday = lubridate::wday(trans_date_trans_time, label = TRUE)
  ) %>% 
  #group by key columns
  group_by(date_only,month_name,hour,weekday) %>% 
  #obtain count for each
  summarise(n=n()) %>% 

  #pivot long to make creation of bar chart easier later
  pivot_longer(cols = c(month_name, hour, weekday), 
    names_to = "variable", 
    values_to = "value", 
    #make all values equal same type to allow for pivot long
    values_transform = list(value = as.character)) %>% 
  
  #create new value column to make sure that month, year, and hour equals the right value type sothat in chart creation, the x axis is able to be ordered appropriately
  mutate(value = case_when(
    variable == "month_name" ~ factor(value, levels = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")),
    variable == "weekday" ~ factor(value, levels = c("Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat")),
    variable == "hour" ~ factor(value, levels = as.character(0:23))
  )) %>% 
 
  #creation of column charts for month, day, hour 
  ggplot(aes(x = value, y = n, fill = variable)) +
    geom_bar(stat = "identity") +
    labs(title = "Transactions by Month, Hour, and Weekday", x = "Category", y = "Number of Transactions") +
    theme_minimal() +
    #facet wrap by month, day, hour
    facet_wrap(~ variable, scales = "free", ncol = 1) +
    theme(legend.position = "none") +
    scale_y_continuous(labels = scales::label_number(accuracy = 1))

```

-   Are older customers significantly more likely to be victims of credit card fraud? To calculate a customer's age, we use the `lubridate` package and the following code

Answer: From the histogram, it appears that the frequency of fraud most commonly appears around age 50 - however, this is just an absolute number and is not necessairly the likelihood of credit card fraud as it could be that people ages 75 have a higher likelihood of CC fraud, but don't really use many credit card transactions thus bringing down the absolute frequency of fraudulent transactions

However, looking at the updated column chart, it appears that for people ages 70-90, the likelihood of each of their CC transactions being fraudulent does appear to be higher than most other age brackets with the 3rd highest bucket being 50-60. Therefore it does appear that older customers do seem to be more likely victims of CC fraud

```         
  mutate(
   age = interval(dob, trans_date_trans_time) / years(1),
    )
```

```{r}

card_fraud %>%
  mutate(
  age = interval(dob, trans_date_trans_time) / years(1),
  ) %>% 
  
  #group by age and obtain count
  filter(is_fraud == 1) %>% 
  group_by(age) %>% 
  summarise(n=n()) %>% 
  
  #create histogram
  ggplot(aes(x = age)) +
  #making free scales so histogram visualistion isn't skewed by axis scale of other
  geom_histogram(binwidth = 10, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Fraud by age",
       x = "Age",
       y = "Frequency") +
  theme_minimal()
  
#create histogram and then obtain summary statistics - % of fraud in each age bucket

  #create new dataframe to round down age to nearest 10 to allow for easier calculation of fraudulent transaction likelihood for each age bucket
age_card_fraud <- card_fraud %>% 
  mutate(
  age = interval(dob, trans_date_trans_time) / years(1),
  ) %>% 
  mutate(age_rounded_down = floor(age / 10) * 10) %>% 
  group_by(age_rounded_down, is_fraud) %>% 
  #this will give the count for each age bucket and fraud status
  summarise(count=n()) %>% 
  mutate(perc = count/sum(count)) %>% 
  filter(is_fraud == 1) 

print(age_card_fraud)

  #column chart for updated dataframe
  age_card_fraud %>% 
  ggplot(aes(x = age_rounded_down, y = perc)) +
  geom_bar(stat = "identity", fill = "blue") +
  labs(title = "Likehood of transaction being fraudulent by age group",
    x = "Age",
    y = "Percentage of Fraudulent Transactions") +
  scale_y_continuous(labels = scales::percent_format()) +
  theme_minimal()

```

-   Is fraud related to distance? The distance between a card holder's home and the location of the transaction can be a feature that is related to fraud. To calculate distance, we need the latidue/longitude of card holders's home and the latitude/longitude of the transaction, and we will use the [Haversine formula](https://en.wikipedia.org/wiki/Haversine_formula) to calculate distance. I adapted code to [calculate distance between two points on earth](https://www.geeksforgeeks.org/program-distance-two-points-earth/amp/) which you can find below

**Answer**: Both the fraud and non-fraud box/violin charts have similar distributions and shapes indicating that is is likely that there is relationship between distance and fraud. Given that the medians and distributions between the two are very similar, distance alone is unlikely related to fraud and it is likely that further factors will need to be accounted for.

```{r}
# distance between card holder's home and transaction
# code adapted from https://www.geeksforgeeks.org/program-distance-two-points-earth/amp/


card_fraud_distance <- card_fraud %>%
  mutate(
    
    # convert latitude/longitude to radians
    lat1_radians = lat / 57.29577951,
    lat2_radians = merch_lat / 57.29577951,
    long1_radians = long / 57.29577951,
    long2_radians = merch_long / 57.29577951,
    
    # calculate distance in miles
    distance_miles = 3963.0 * acos((sin(lat1_radians) * sin(lat2_radians)) + cos(lat1_radians) * cos(lat2_radians) * cos(long2_radians - long1_radians)),

    # calculate distance in km
    distance_km = 6377.830272 * acos((sin(lat1_radians) * sin(lat2_radians)) + cos(lat1_radians) * cos(lat2_radians) * cos(long2_radians - long1_radians))

  )

card_fraud_distance %>% 
  ggplot(aes(x = factor(is_fraud), y = distance_km, fill = factor(is_fraud))) +
  geom_boxplot() +
  labs(title = "Boxplot of Distance by Fraud Status",
       x = "Fraud Status (0 = No, 1 = Yes)",
       y = "Distance") +
  scale_fill_manual(values = c("0" = "skyblue", "1" = "orange")) +
  theme_minimal() +
  theme(legend.position = "none")


card_fraud_distance %>% 
  ggplot(aes(x = factor(is_fraud), y = distance_km, fill = factor(is_fraud))) +
  geom_violin() +
  labs(title = "Violin Plot of Distance by Fraud Status",
       x = "Fraud Status (0 = No, 1 = Yes)",
       y = "Distance") +
  scale_fill_manual(values = c("0" = "skyblue", "1" = "orange")) +
  theme_minimal() +
  theme(legend.position = "none")


```

Plot a boxplot or a violin plot that looks at the relationship of distance and `is_fraud`. Does distance seem to be a useful feature in explaining fraud?

# Exploring sources of electricity production, CO2 emissions, and GDP per capita.

We will get energy data from the Our World in Data website, and CO2 and GDP per capita emissions from the World Bank, using the `wbstats`package.

```{r}
#| message: false
#| warning: false

# Download electricity data
url <- "https://nyc3.digitaloceanspaces.com/owid-public/data/energy/owid-energy-data.csv"

energy <- read_csv(url) %>% 
  filter(year >= 1990) %>% 
  drop_na(iso_code) %>% 
  select(1:3,
         biofuel = biofuel_electricity,
         coal = coal_electricity,
         gas = gas_electricity,
         hydro = hydro_electricity,
         nuclear = nuclear_electricity,
         oil = oil_electricity,
         other_renewable = other_renewable_exc_biofuel_electricity,
         solar = solar_electricity,
         wind = wind_electricity, 
         electricity_demand,
         electricity_generation,
         net_elec_imports,	# Net electricity imports, measured in terawatt-hours
         energy_per_capita,	# Primary energy consumption per capita, measured in kilowatt-hours	Calculated by Our World in Data based on BP Statistical Review of World Energy and EIA International Energy Data
         energy_per_gdp,	# Energy consumption per unit of GDP. This is measured in kilowatt-hours per 2011 international-$.
         per_capita_electricity, #	Electricity generation per capita, measured in kilowatt-hours
  ) 

# Download data for C02 emissions per capita https://data.worldbank.org/indicator/EN.ATM.CO2E.PC
co2_percap <- wb_data(country = "countries_only", 
                      indicator = "EN.ATM.CO2E.PC", 
                      start_date = 1990, 
                      end_date = 2022,
                      return_wide=FALSE) %>% 
  filter(!is.na(value)) %>% 
  #drop unwanted variables
  select(-c(unit, obs_status, footnote, last_updated)) %>% 
  rename(year = date,
         co2percap = value)


# Download data for GDP per capita  https://data.worldbank.org/indicator/NY.GDP.PCAP.PP.KD
gdp_percap <- wb_data(country = "countries_only", 
                      indicator = "NY.GDP.PCAP.PP.KD", 
                      start_date = 1990, 
                      end_date = 2022,
                      return_wide=FALSE) %>% 
  filter(!is.na(value)) %>% 
  #drop unwanted variables
  select(-c(unit, obs_status, footnote, last_updated)) %>% 
  rename(year = date,
         GDPpercap = value)
```

There are many sources of data on how countries generate their electricity and their CO2 emissions. I would like you to create three graphs:

## 1. A stacked area chart that shows how your own country generated its electricity since 2000.

You will use

`geom_area(colour="grey90", alpha = 0.5, position = "fill")`

```{r}

NZ_energy <- energy %>% 
  #filter for New Zealand, and years above 2000 (inclusive)
  filter(country == "New Zealand" & year >= 2000) %>% 
  #pivot long for columns that include generation type which is between columns 4-12
  pivot_longer(
    cols = 4:12,
    names_to = "Generation_Type",
    values_to = "Generation"
  ) 
  
  print(NZ_energy)
  
  #create 100% stacked area chart
NZ_energy %>% 
  ggplot(aes(x = year, y = Generation, fill = Generation_Type)) +
  geom_area(colour="grey90", alpha = 0.5, position = "fill") +
  labs(title = "New Zealand Electricity Generation by Method",
       x = "Year",
       y = "Electricity Generation %",
       fill = "Generation Method") +
  #make y axis labels percent
  scale_y_continuous(labels = scales::percent_format()) +
  theme_minimal()

  #create standard stacked area chart
NZ_energy %>% 
  ggplot(aes(x = year, y = Generation, fill = Generation_Type)) +
  geom_area(colour="grey90", alpha = 0.5, position = "stack") +
  labs(title = "New Zealand Electricity Generation by Method",
       x = "Year",
       y = "Electricity Generation (TwH)",
       fill = "Generation Method") +
  theme_minimal()



```

## 2. A scatter plot that looks at how CO2 per capita and GDP per capita are related

```{r}

#first join the co2 listing to the gdp listing
co2_gdp <- left_join(co2_percap, gdp_percap, by = c("iso3c", "year")) %>% 
  #select only relevant columns
  select(year, iso3c, co2percap, GDPpercap) %>% 
  group_by(year) %>% 
  summarise(
    ave_co2percap = mean(co2percap, na.rm = TRUE),
    ave_GDPpercap = mean(GDPpercap, na.rm = TRUE)) %>% 
  print()
  

# scatter plot showing relationship between the two from 2015
co2_gdp %>%

  ggplot(aes(x = ave_GDPpercap, y = ave_co2percap, label = year)) +
  geom_point(size = 3) +
  geom_text(vjust = -0.5, hjust = 0.5) +  # Adding data labels
  labs(title = "Relationship between CO2 per Capita and GDP per Capita Over the Years",
       x = "GDP per Capita",
       y = "CO2 per Capita",
  ) +
  theme_minimal() +
  theme(legend.position = "none")



```

## 3. A scatter plot that looks at how electricity usage (kWh) per capita/day GDP per capita are related

```{r}

energy_updated <- energy %>% 
  rename(iso3c = iso_code) %>% 
  # add column for energy per capita per day
  mutate(energy_per_capita_day = 
    energy_per_capita/
    #giving number of days in each given year (accounts for lear year)
    #returns day of year between 1 - 366
    yday(
    #converts date string to date object
    ymd(
    #concatenates "12-31" to year 
    paste0(year, "-12-31")))) %>% 
  #select relevant columns
  select(iso3c, year, energy_per_capita_day) %>% 
  print() 
  
#left join gdp per capita to updated energy dataframe
gdp_percap %>% 
  left_join(energy_updated, by = c("iso3c", "year")) %>% 
  select(iso3c, year, energy_per_capita_day, GDPpercap) %>% 
  group_by(year) %>% 
  summarise(
    ave_energy_per_capday = mean(energy_per_capita_day, na.rm = TRUE),
    ave_GDPpercap = mean(GDPpercap, na.rm = TRUE)
  ) %>% 
  
# scatter plot showing relationship between the two
  ggplot(aes(x = ave_energy_per_capday, y = ave_GDPpercap, label = year)) +
  geom_point(size = 3) +
  geom_text(vjust = -0.5, hjust = 0.5) +  # Adding data labels
  labs(title = "Relationship between GDP per Capita and Energy per Capita/Day Over the Years",
       x = "Energy per Capita/day",
       y = "GDP per Capita") +
  theme_minimal() +
  theme(legend.position = "none")



```




Specific questions:

1.  How would you turn `energy` to long, tidy format?
2.  You may need to join these data frames
    -   Use `left_join` from `dplyr` to [join the tables](http://r4ds.had.co.nz/relational-data.html)
    -   To complete the merge, you need a unique *key* to match observations between the data frames. Country names may not be consistent among the three dataframes, so please use the 3-digit ISO code for each country
    -   An aside: There is a great package called [`countrycode`](https://github.com/vincentarelbundock/countrycode) that helps solve the problem of inconsistent country names (Is it UK? United Kingdom? Great Britain?). `countrycode()` takes as an input a country's name in a specific format and outputs it using whatever format you specify.
3.  Write a function that takes as input any country's name and returns all three graphs. You can use the `patchwork` package to arrange the three graphs as shown below

```{r, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "electricity-co2-gdp.png"), error = FALSE)
```

# Details

-   Who did you collaborate with: TYPE NAMES HERE
-   Approximately how much time did you spend on this problem set: ANSWER HERE
-   What, if anything, gave you the most trouble: ANSWER HERE

**Please seek out help when you need it,** and remember the [15-minute rule](https://dsb2023.netlify.app/syllabus/#the-15-minute-rule){target="_blank"}. You know enough R (and have enough examples of code from class and your readings) to be able to do this. If you get stuck, ask for help from others, post a question on Slack-- and remember that I am here to help too!

> As a true test to yourself, do you understand the code you submitted and are you able to explain it to someone else?

# Rubric

13/13: Problem set is 100% completed. Every question was attempted and answered, and most answers are correct. Code is well-documented (both self-documented and with additional comments as necessary). Used tidyverse, instead of base R. Graphs and tables are properly labelled. Analysis is clear and easy to follow, either because graphs are labeled clearly or you've written additional text to describe how you interpret the output. Multiple Github commits. Work is exceptional. I will not assign these often.

8/13: Problem set is 60--80% complete and most answers are correct. This is the expected level of performance. Solid effort. Hits all the elements. No clear mistakes. Easy to follow (both the code and the output). A few Github commits.

5/13: Problem set is less than 60% complete and/or most answers are incorrect. This indicates that you need to improve next time. I will hopefully not assign these often. Displays minimal effort. Doesn't complete all components. Code is poorly written and not documented. Uses the same type of plot for each graph, or doesn't use plots appropriate for the variables being analyzed. No Github commits.
